{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71342313",
   "metadata": {},
   "source": [
    "# 05 - Results Analysis & Visualization\n",
    "\n",
    "This notebook provides tools for analyzing MedJEPA training results and\n",
    "visualizing what the model has learned.\n",
    "\n",
    "**Sections:**\n",
    "1. Training History\n",
    "2. Embedding Space Visualization (t-SNE)\n",
    "3. Attention Maps\n",
    "4. Data Efficiency Curves\n",
    "5. Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d595f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend if needed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from medjepa.utils.visualization import (\n",
    "    plot_training_history,\n",
    "    plot_embedding_space,\n",
    "    plot_attention_map,\n",
    "    plot_data_efficiency,\n",
    "    extract_attention_weights,\n",
    "    plot_evaluation_summary,\n",
    ")\n",
    "from medjepa.utils.device import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3663d",
   "metadata": {},
   "source": [
    "## 1. Training History\n",
    "\n",
    "Load and plot the loss curve from pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "history_path = '../checkpoints/training_history.json'\n",
    "if os.path.exists(history_path):\n",
    "    with open(history_path) as f:\n",
    "        history = json.load(f)\n",
    "    print(f\"Epochs: {len(history['epochs'])}\")\n",
    "    print(f\"Final loss: {history['train_loss'][-1]:.6f}\")\n",
    "    plot_training_history(history, save_path='../results/training_history.png')\n",
    "else:\n",
    "    print(f'No training history found at {history_path}')\n",
    "    print('Run pre-training first: python scripts/pretrain.py ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a0889",
   "metadata": {},
   "source": [
    "## 2. Embedding Space (t-SNE)\n",
    "\n",
    "Visualize how the model organises different disease types in its\n",
    "embedding space. Good representations should cluster similar\n",
    "diseases together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f482ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medjepa.models import LeJEPA, ViTEncoder\n",
    "from medjepa.evaluation import LinearProbeEvaluator\n",
    "from medjepa.data.datasets import MedicalImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Configure these paths ---\n",
    "CHECKPOINT = '../checkpoints/best_model.pt'\n",
    "DATA_DIR   = '../data/raw/ham10000'\n",
    "CSV_PATH   = '../data/raw/ham10000/HAM10000_metadata.csv'\n",
    "LABEL_COL  = 'dx'\n",
    "# ---\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(CHECKPOINT, map_location='cpu', weights_only=False)\n",
    "cfg  = ckpt.get('config', {})\n",
    "\n",
    "embed_dim  = cfg.get('embed_dim', 768)\n",
    "enc_depth  = cfg.get('encoder_depth', 12)\n",
    "pred_depth = cfg.get('predictor_depth', 6)\n",
    "image_size = cfg.get('image_size', 224)\n",
    "patch_size = cfg.get('patch_size', 16)\n",
    "\n",
    "print(f'Model: embed_dim={embed_dim}, encoder_depth={enc_depth}, '\n",
    "      f'predictor_depth={pred_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5584ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and load weights\n",
    "encoder = ViTEncoder(\n",
    "    image_size=image_size, patch_size=patch_size,\n",
    "    embed_dim=embed_dim, depth=enc_depth,\n",
    ")\n",
    "model = LeJEPA(\n",
    "    encoder=encoder, embed_dim=embed_dim,\n",
    "    predictor_depth=pred_depth,\n",
    ")\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model = model.to(device).eval()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = MedicalImageDataset(\n",
    "    data_dir=DATA_DIR, metadata_csv=CSV_PATH,\n",
    "    label_column=LABEL_COL, image_size=image_size,\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "num_classes = len(set(dataset.labels))\n",
    "class_names = None\n",
    "if hasattr(dataset, 'label_map'):\n",
    "    inv_map = {v: k for k, v in dataset.label_map.items()}\n",
    "    class_names = [inv_map[i] for i in range(num_classes)]\n",
    "\n",
    "print(f'Dataset: {len(dataset)} images, {num_classes} classes')\n",
    "if class_names:\n",
    "    print(f'Classes: {class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc831abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "evaluator = LinearProbeEvaluator(model)\n",
    "features, labels = evaluator.extract_features(loader)\n",
    "print(f'Extracted: features {features.shape}, labels {labels.shape}')\n",
    "\n",
    "# t-SNE plot\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "plot_embedding_space(\n",
    "    features, labels, class_names=class_names,\n",
    "    title=f'MedJEPA Embedding Space ({len(dataset)} images)',\n",
    "    save_path='../results/embedding_tsne.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daded73c",
   "metadata": {},
   "source": [
    "## 3. Attention Maps\n",
    "\n",
    "Visualize where the model focuses its attention. Clinicians can use\n",
    "this to verify the model looks at the right areas (lesion, not background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8207d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Pick a few sample images\n",
    "sample_indices = [0, 100, 500, 1000]\n",
    "for idx in sample_indices:\n",
    "    if idx >= len(dataset):\n",
    "        continue\n",
    "    img_tensor, label = dataset[idx]\n",
    "\n",
    "    # Get attention map\n",
    "    attn_map = extract_attention_weights(model, img_tensor)\n",
    "\n",
    "    # Resize attention to image size\n",
    "    attn_resized = cv2.resize(\n",
    "        attn_map, (image_size, image_size),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    )\n",
    "\n",
    "    # Convert tensor image to numpy for display\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
    "\n",
    "    label_name = class_names[label] if class_names else f'Class {label}'\n",
    "    plot_attention_map(\n",
    "        img_np, attn_resized,\n",
    "        title=f'Sample {idx} — {label_name}',\n",
    "        save_path=f'../results/attention_sample_{idx}.png',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a636a",
   "metadata": {},
   "source": [
    "## 4. Data Efficiency Curve\n",
    "\n",
    "Load few-shot / data-efficiency evaluation results and plot the\n",
    "\"money plot\" — how accuracy scales with labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results if available\n",
    "eval_path = '../results/evaluation_results.json'\n",
    "if os.path.exists(eval_path):\n",
    "    with open(eval_path) as f:\n",
    "        eval_results = json.load(f)\n",
    "\n",
    "    # Data efficiency results\n",
    "    if 'data_efficiency' in eval_results:\n",
    "        plot_data_efficiency(\n",
    "            eval_results['data_efficiency'],\n",
    "            title='MedJEPA Data Efficiency (HAM10000)',\n",
    "            save_path='../results/data_efficiency.png',\n",
    "        )\n",
    "    else:\n",
    "        print('No data_efficiency key in results. '\n",
    "              'Run: python scripts/evaluate.py ...')\n",
    "else:\n",
    "    print(f'No evaluation results at {eval_path}')\n",
    "    print('Run: python scripts/evaluate.py --checkpoint ... --data_dir ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc62bbe",
   "metadata": {},
   "source": [
    "## 5. Evaluation Summary\n",
    "\n",
    "Bar chart of all evaluation metrics at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(eval_path):\n",
    "    with open(eval_path) as f:\n",
    "        eval_results = json.load(f)\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    # Linear probe accuracy\n",
    "    if 'linear_probe' in eval_results:\n",
    "        lp = eval_results['linear_probe']\n",
    "        if 'accuracy' in lp:\n",
    "            summary['Linear Probe'] = lp['accuracy']\n",
    "\n",
    "    # Few-shot results\n",
    "    if 'few_shot' in eval_results:\n",
    "        for key, val in eval_results['few_shot'].items():\n",
    "            if isinstance(val, dict) and 'accuracy' in val:\n",
    "                summary[key] = val['accuracy']\n",
    "\n",
    "    if summary:\n",
    "        plot_evaluation_summary(\n",
    "            summary,\n",
    "            title='MedJEPA Evaluation Summary',\n",
    "            save_path='../results/evaluation_summary.png',\n",
    "        )\n",
    "    else:\n",
    "        print('No summary metrics found in results.')\n",
    "else:\n",
    "    print('Run evaluation first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd18623",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Generated plots are saved to `results/` folder.**\n",
    "\n",
    "For full evaluation, run:\n",
    "```bash\n",
    "python scripts/evaluate.py \\\n",
    "    --checkpoint checkpoints/best_model.pt \\\n",
    "    --data_dir data/raw/ham10000 \\\n",
    "    --metadata_csv data/raw/ham10000/HAM10000_metadata.csv \\\n",
    "    --label_column dx --num_classes 7\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
