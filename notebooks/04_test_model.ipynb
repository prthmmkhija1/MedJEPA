{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd915641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from medjepa.models.lejepa import LeJEPA\n",
    "from medjepa.utils.device import get_device_info, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6217929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEVICE INFORMATION\n",
      "==================================================\n",
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "MPS available: False\n",
      "Using CPU (no GPU detected)\n",
      "Selected device: cpu\n",
      "==================================================\n",
      "Using CPU (no GPU detected)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check your machine\n",
    "get_device_info()\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e944ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 12,538,944\n",
      "That's 12.5 million parameters\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create a small model for testing\n",
    "# Using smaller dimensions so it runs fast on CPU\n",
    "model = LeJEPA(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dim=384,        # Smaller than default 768\n",
    "    encoder_depth=6,      # Fewer layers than default 12\n",
    "    encoder_heads=6,\n",
    "    predictor_dim=192,\n",
    "    predictor_depth=3,\n",
    "    predictor_heads=3,\n",
    "    mask_ratio=0.75,\n",
    "    lambda_reg=1.0,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"That's {total_params / 1e6:.1f} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732fae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.0083\n",
      "Prediction loss: 0.0052\n",
      "Regularization loss: 0.0031\n",
      "\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test forward pass with fake data\n",
    "# Create fake batch of 4 images\n",
    "fake_images = torch.randn(4, 3, 224, 224).to(device)\n",
    "\n",
    "# Run forward pass\n",
    "model.train()\n",
    "losses = model(fake_images)\n",
    "\n",
    "print(f\"Total loss: {losses['total_loss'].item():.4f}\")\n",
    "print(f\"Prediction loss: {losses['prediction_loss'].item():.4f}\")\n",
    "print(f\"Regularization loss: {losses['regularization_loss'].item():.4f}\")\n",
    "print(\"\\nForward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66194e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([4, 384])\n",
      "Each image is represented by a vector of 384 numbers\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test encoding (what you'd use for downstream tasks)\n",
    "model.eval()\n",
    "embeddings = model.encode(fake_images)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # Should be (4, 384)\n",
    "print(f\"Each image is represented by a vector of {embeddings.shape[1]} numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa50f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pass successful! Model can learn.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test that gradients flow (model can learn)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "losses = model(fake_images)\n",
    "losses[\"total_loss\"].backward()  # Compute gradients\n",
    "optimizer.step()                  # Update weights\n",
    "optimizer.zero_grad()             # Reset gradients\n",
    "\n",
    "print(\"Backward pass successful! Model can learn.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
