{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd915641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from medjepa.models.lejepa import LeJEPA\n",
    "from medjepa.utils.device import get_device_info, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6217929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEVICE INFORMATION\n",
      "==================================================\n",
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "MPS available: False\n",
      "Using CPU (no GPU detected)\n",
      "Selected device: cpu\n",
      "==================================================\n",
      "Using CPU (no GPU detected)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check your machine\n",
    "get_device_info()\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e944ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 12,538,944\n",
      "That's 12.5 million parameters\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create a small model for testing\n",
    "# Using smaller dimensions so it runs fast on CPU\n",
    "model = LeJEPA(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dim=384,        # Smaller than default 768\n",
    "    encoder_depth=6,      # Fewer layers than default 12\n",
    "    encoder_heads=6,\n",
    "    predictor_dim=192,\n",
    "    predictor_depth=3,\n",
    "    predictor_heads=3,\n",
    "    mask_ratio=0.75,\n",
    "    lambda_reg=1.0,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"That's {total_params / 1e6:.1f} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732fae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.0083\n",
      "Prediction loss: 0.0052\n",
      "Regularization loss: 0.0031\n",
      "\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test forward pass with fake data\n",
    "# Create fake batch of 4 images\n",
    "fake_images = torch.randn(4, 3, 224, 224).to(device)\n",
    "\n",
    "# Run forward pass\n",
    "model.train()\n",
    "losses = model(fake_images)\n",
    "\n",
    "print(f\"Total loss: {losses['total_loss'].item():.4f}\")\n",
    "print(f\"Prediction loss: {losses['prediction_loss'].item():.4f}\")\n",
    "print(f\"Regularization loss: {losses['regularization_loss'].item():.4f}\")\n",
    "print(\"\\nForward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66194e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([4, 384])\n",
      "Each image is represented by a vector of 384 numbers\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test encoding (what you'd use for downstream tasks)\n",
    "model.eval()\n",
    "embeddings = model.encode(fake_images)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # Should be (4, 384)\n",
    "print(f\"Each image is represented by a vector of {embeddings.shape[1]} numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa50f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pass successful! Model can learn.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test that gradients flow (model can learn)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "losses = model(fake_images)\n",
    "losses[\"total_loss\"].backward()  # Compute gradients\n",
    "optimizer.step()                  # Update weights\n",
    "optimizer.zero_grad()             # Reset gradients\n",
    "\n",
    "print(\"Backward pass successful! Model can learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e868ef",
   "metadata": {},
   "source": [
    "## V-JEPA Model Test (3D Volumes)\n",
    "\n",
    "Test the V-JEPA architecture used for BraTS and Decathlon 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b988143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: V-JEPA model â€” forward pass test\n",
    "from medjepa.models.vjepa import VJEPA\n",
    "\n",
    "vjepa = VJEPA(\n",
    "    volume_size=(64, 64, 32),   # Smaller for testing\n",
    "    patch_size=(16, 16, 8),\n",
    "    in_channels=1,\n",
    "    embed_dim=384,\n",
    "    depth=4,\n",
    "    num_heads=6,\n",
    "    predictor_dim=192,\n",
    "    predictor_depth=2,\n",
    "    mask_ratio=0.75,\n",
    "    lambda_reg=1.0,\n",
    ").to(device)\n",
    "\n",
    "total_params_v = sum(p.numel() for p in vjepa.parameters())\n",
    "print(f\"V-JEPA parameters: {total_params_v:,} ({total_params_v/1e6:.1f}M)\")\n",
    "\n",
    "# Forward pass with fake 3D volume\n",
    "fake_volume = torch.randn(2, 1, 64, 64, 32).to(device)\n",
    "vjepa.train()\n",
    "v_losses = vjepa(fake_volume)\n",
    "print(f\"V-JEPA Total loss:      {v_losses['total_loss'].item():.4f}\")\n",
    "print(f\"V-JEPA Prediction loss: {v_losses['prediction_loss'].item():.4f}\")\n",
    "print(f\"V-JEPA Reg loss:        {v_losses['regularization_loss'].item():.4f}\")\n",
    "\n",
    "# Test 3D encoding\n",
    "vjepa.eval()\n",
    "v_emb = vjepa.encode(fake_volume)\n",
    "print(f\"V-JEPA embedding shape: {v_emb.shape}\")  # (2, 384)\n",
    "print(\"V-JEPA forward + encode successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828cf911",
   "metadata": {},
   "source": [
    "## Load Pre-trained Checkpoint\n",
    "\n",
    "Load a real pre-trained model from `checkpoints/` and verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load pre-trained LeJEPA checkpoint\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = \"../checkpoints/best_model.pt\"\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    cfg = ckpt.get(\"config\", {})\n",
    "    print(\"Checkpoint keys:\", list(ckpt.keys()))\n",
    "    print(\"Config:\", cfg)\n",
    "\n",
    "    # Reconstruct model from checkpoint config\n",
    "    loaded_model = LeJEPA(\n",
    "        image_size=cfg.get(\"image_size\", 224),\n",
    "        patch_size=cfg.get(\"patch_size\", 16),\n",
    "        embed_dim=cfg.get(\"embed_dim\", 768),\n",
    "        encoder_depth=cfg.get(\"encoder_depth\", 12),\n",
    "        predictor_depth=cfg.get(\"predictor_depth\", 6),\n",
    "    )\n",
    "    loaded_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    loaded_model = loaded_model.to(device).eval()\n",
    "\n",
    "    total_params_loaded = sum(p.numel() for p in loaded_model.parameters())\n",
    "    print(f\"\\nLoaded model: {total_params_loaded:,} parameters\")\n",
    "\n",
    "    if \"epoch\" in ckpt:\n",
    "        print(f\"Trained for {ckpt['epoch']} epochs\")\n",
    "    if \"loss\" in ckpt:\n",
    "        print(f\"Best loss: {ckpt['loss']:.6f}\")\n",
    "\n",
    "    # Quick test with fake image\n",
    "    test_img = torch.randn(1, 3, cfg.get(\"image_size\", 224), cfg.get(\"image_size\", 224)).to(device)\n",
    "    emb = loaded_model.encode(test_img)\n",
    "    print(f\"Embedding shape: {emb.shape}\")\n",
    "    print(\"Pre-trained checkpoint loaded and verified!\")\n",
    "else:\n",
    "    print(f\"No checkpoint at {CHECKPOINT_PATH}\")\n",
    "    print(\"Run pre-training first: python scripts/run_gpu_full.py\")\n",
    "\n",
    "# Check V-JEPA checkpoint too\n",
    "VJEPA_CKPT = \"../checkpoints/best_vjepa_model.pt\"\n",
    "if os.path.exists(VJEPA_CKPT):\n",
    "    v_ckpt = torch.load(VJEPA_CKPT, map_location=device, weights_only=False)\n",
    "    print(f\"\\nV-JEPA checkpoint also found! Epoch: {v_ckpt.get('epoch')}, Loss: {v_ckpt.get('loss', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"\\nNo V-JEPA checkpoint yet at {VJEPA_CKPT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
